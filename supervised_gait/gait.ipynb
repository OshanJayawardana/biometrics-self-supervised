{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OshanJayawardana/biometrics-self-supervised/blob/main/supervised_gait/gait.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget \"https://data.mendeley.com/api/datasets-v2/datasets/fwhn8hmz4f/zip/download?version=2\" -c -O \"gait.zip\"\n",
    "! unzip \"gait.zip\" -d \"gait_dataset/\"\n",
    "! rm \"gait.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Concatenate, BatchNormalization, ReLU, Add, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout\n",
    "\n",
    "from backbones import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = 50\n",
    "x_train=np.array([])\n",
    "y_train=[]\n",
    "count=0\n",
    "path = \"/content/gait_dataset/IDNet's dataset/user_coordinates\"\n",
    "for user_id in range(1,51):\n",
    "  for session_id in range(1,16):\n",
    "    try:\n",
    "      filename = \"u\"+str(user_id).rjust(3, '0')+\"_w\"+str(session_id).rjust(3, '0')+\"_data_user_coord.csv\"\n",
    "      data = pd.read_csv(os.path.join(path,filename))\n",
    "      data = np.array(data)\n",
    "      data=np.lib.stride_tricks.sliding_window_view(data, (frame_size,data.shape[1]))[::frame_size//2, :]\n",
    "      data=data.reshape(data.shape[0],data.shape[2],data.shape[3])\n",
    "      count+=data.shape[0]\n",
    "      if x_train.shape[0]==0:\n",
    "        x_train  = data\n",
    "        y_train += [user_id-1]*data.shape[0]\n",
    "      else:\n",
    "        x_train  = np.concatenate((x_train,data), axis=0)\n",
    "        y_train += [user_id-1]*data.shape[0]\n",
    "    except FileNotFoundError:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norma(x_all):\n",
    "  x = np.reshape(x_all,(x_all.shape[0]*x_all.shape[1],x_all.shape[2]))\n",
    "  scaler = StandardScaler()\n",
    "  x = scaler.fit_transform(x)\n",
    "  x_all = np.reshape(x,(x_all.shape[0],x_all.shape[1],x_all.shape[2]))\n",
    "  x=[]\n",
    "  return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = norma(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, np.array(y_train), test_size=0.4)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = 3\n",
    "con =4\n",
    "num_classes=50\n",
    "inputs = Input(shape=(50,3))\n",
    "x = Conv1D(filters=16*con,kernel_size=ks,strides=1, padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = MaxPooling1D(pool_size=4, strides=4)(x)\n",
    "x = Dropout(rate=0.1)(x)\n",
    "x = resnetblock(x, CR=32*con, KS=ks)\n",
    "x = resnetblock(x, CR=32*con, KS=ks)\n",
    "x = resnetblock(x, CR=64*con, KS=ks)\n",
    "x = resnetblock(x, CR=64*con, KS=ks)\n",
    "x = resnetblock(x, CR=128*con, KS=ks)\n",
    "x = resnetblock(x, CR=128*con, KS=ks)\n",
    "x = resnetblock_final(x, CR=128*con, KS=ks)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "resnettssd = Model(inputs, outputs)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', restore_best_weights=True, patience=5)\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.0001, decay_rate=0.5, decay_steps=1000000)# 0.0001, 0.9, 100000\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "resnettssd.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'] )\n",
    "history = resnettssd.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, callbacks=callback, batch_size=128)\n",
    "\n",
    "results = resnettssd.evaluate(x_test,y_test)\n",
    "print(\"test acc:\", results[1])\n",
    "\n",
    "#Calculating kappa score\n",
    "metric = tfa.metrics.CohenKappa(num_classes=num_classes, sparse_labels=True)\n",
    "metric.update_state(y_true=y_test , y_pred=resnettssd.predict(x_test))\n",
    "result = metric.result()\n",
    "print('kappa score: ',result.numpy())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d12bc326566d5b9b477ded079e05d144fdeac165de302909c494c00ce8896e7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
